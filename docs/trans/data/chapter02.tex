\section{学习马氏距离}
研究基于马氏距离函数的距离和相似性度量在计算机视觉领域得到了很多的关注。总体来讲，马氏距离衡量了两点 $\mathbf x_i$和$\mathbf x_j$ 之间的平方距离

$$ d^{2}_{M}(x_{i},x_{j}) = (x_{i}-x_{j})^{T}M(x_{i}-x_{j}) $$
其中  是一个半正定矩阵，而  是一对例子。另外，在接下来的讨论中，我们利用    标识两个实例为相似的一对，他们共享相同的类别标识（）而且（）。为了解释我们的方法，我们下面给出目前在马氏尺度衡量最先进的算法简介。尤其是我们检验了LMNN，ITML和LDML。
\subsection{大边缘最近邻尺度}
Weinberger地方法目标在于通过利用周围数据的结构来提高K-nn分类器。对于每一个实例，一个包围着最近的拥有相同标识的K个邻居近邻区域被建立。拥有不同标识并且侵入这片区域的实例将会被处罚。下面这个目标函数解释了这个规则：

$$ \epsilon(M) = \sum_{j \leadsto i}[d^{2}_{M}(x_i,x_j)+\mu\sum_{l}(1 - y_{il})\xi_{ijl}(M)]$$

第一个变量使脚标为$j\leadsto i$目标近邻对$\mathbf x_i$,$\mathbf x_j$最小。

$$ \xi_{ijl}(M) = 1 + d_{M}^2(x_{i},x_{j}) - d_{M}^{2}(x_{i},x_{l})$$
为了估计Ｍ，梯度下降定义一个含有正项差异的三元组（I,j,l）使之按照梯度运行。
$$ \frac{\partial \epsilon (M_t)}{\partial M^t} = \sum_{j\leadsto i}C_ij + \mu \sum_{(i,j,l)}(C_{ij} - C_{il})$$

其中表示成对间差异的外积。概念上来讲，对于比较活跃的三元组，这个公式加强了目标邻居间的联系，减弱了与侵入者的联系。
\subsection{信息论度量学习}
Davis研究了多变量高斯分布和马氏距离之间的联系。思想在于寻找一个解决方案能够平衡约束的满足同时接近先验的距离度量M0。例如欧式距离中的单位矩阵。解决方案的接近都通过相关联区域的Kullback-Leibler散度。先验可以认为是为了避免过拟合而进行的规则化的过程。约束强制使相同的点对在某一个距离   而不同的点对超过某一个距离   。优化建立与布雷格曼映射，它将当前的解决方案通过要给更新规则映射到一个单个约束：

$$ M_{t+1} = M_t + \beta M_{t}C_{ij}M_{t}$$

参数$\beta$包含了点对的标识和步长的大笑。它在相同的点对中为正在不同的点对中为负。所以，对于相同的点对，优化向  的方向进化而对于不同的点对则向相反的方向进化。每一对在梯度递减上的影响被概率所控制。
\subsection{线性判别度量}
未完成

如果我们概括一下以上所提到的尺度衡量方法的属性和特征，我们会发现两个共同点。第一，所有的方法依赖于迭代优化的过程，它在大规模数据集的情况下计算会变得花费很大。第二，如果我们比较一下  给出的不同方法中的更新规则，我们可以发现对于相同的点对，优化总是向  的方向进行而对于不同的点对则向相反方向。在接下来，我们介绍一个不用叠戴的公式，他可以在成对之间的差异上建立一个数学推导公式。这允许我们能够面对额外的可扩展性和等价约束能力的挑战。我们参数自由的方法在训练时非常的有效，可以继续研究持续增长的数据的尺度度量。


