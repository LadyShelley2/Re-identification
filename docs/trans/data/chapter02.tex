\section{马氏距离}
研究基于马氏距离函数的距离和相似性度量在计算机视觉领域得到了很多的关注。总体来讲，马氏距离衡量了两点 $\mathbf x_i$和$\mathbf x_j$ 之间的平方距离

$$ d^{2}_{\mathbf M}(x_{i},x_{j}) = (\mathbf x_{i}-\mathbf x_{j})^{T}\mathbf M(\mathbf x_{i}-\mathbf x_{j}) \eqno{(1)}$$
其中$ \mathbf M \succeq 0$  是一个半正定矩阵，而$ \mathbf x_{i}, \mathbf x_{j} \in \mathbf R_{0}$  是一对例子$(i,j)$。另外，在接下来的讨论中，我们利用$y_{ij}$:若$ y_{ij} =0 $表示两个实例为相似的一对，他们有相同的类别标识（$ y_{i} = y_{j}$）而且$ y_{ij}=0$。为了解释我们的方法，我们下面给出目前在马氏尺度衡量最先进的算法简介。尤其是我们检验了LMNN\cite{weinberger2009distance,weinberger2008fast}，ITML\cite{davis2007information}和LDML\cite{guillaumin2009you}。
\subsection{大边缘最近邻尺度}
Weinberger的方法\cite{weinberger2009distance,weinberger2008fast}目标在于通过利用周围数据的结构来提高K-nn分类器。对于每一个实例，一个包围着最近的拥有相同标识的$ k$个邻居近邻区域被建立。拥有不同标识并且侵入这片区域的实例将会被处罚。下面这个目标函数解释了这个规则：

$$ \epsilon(\mathbf M) = \sum_{j \leadsto i}[d^{2}_{\mathbf M}(\mathbf x_i,\mathbf x_j)+\mu\sum_{l}(1 - \mathbf y_{il})\xi_{ijl}(\mathbf M)] \eqno{(2)}$$

第一个变量使脚标为$j\leadsto i$目标近邻对$\mathbf x_i$,$\mathbf x_j$最小。第二个变量考虑了侵入$i$和$j$的半径的冒充者数量。冒充者$l$有一个不同的标签输入，它有一个正的松弛变量$ \epsilon_{ijl}(\mathbf M)\geq 0$:

$$ \xi_{ijl}(\mathbf M) = 1 + d_{\mathbf M}^2(\mathbf x_{i},\mathbf x_{j}) - d_{\mathbf M}^{2}(\mathbf x_{i},\mathbf x_{l})\eqno{(3)}$$
为了估计Ｍ，梯度下降定义一个含有正项差异的三元组（$ i, j, l$）使之按照梯度运行。
$$ \frac{\partial \epsilon (\mathbf M_t)}{\partial \mathbf M^t} = \sum_{j\leadsto i}\mathbf C_ij + \mu \sum_{(i,j,l)}(\mathbf C_{ij} - \mathbf C_{il}) \eqno{(4)}$$

其中表示成对间差异的外积。概念上来讲，对于比较活跃的三元组，这个公式加强了目标邻居间的联系，减弱了与侵入者的联系。

\subsection{信息论度量学习}

Davis\cite{davis2007information}研究了多变量高斯分布和马氏距离之间的联系。思想在于寻找一个解决方案能够平衡约束的满足同时接近先验的距离度量$\mathbf M_{0}$。例如欧式距离中的单位矩阵。解决方案的接近都通过相关联区域的Kullback-Leibler散度。先验可以认为是为了避免过拟合而进行的规则化的过程。约束强制使相同的点对在某一个距离 $d_{\mathbf M}^2(\mathbf x_{i},\mathbf x_{j})\le \mu $  而不同的点对超过某一个距离 $d_{\mathbf M}^2(\mathbf x_{i},\mathbf x_{j})\ge l $   。优化建立与布雷格曼映射，它将当前的解决方案通过要给更新规则映射到一个单个约束：

$$ \mathbf M_{t+1} = \mathbf M_t + \beta \mathbf M_{t}\mathbf C_{ij}\mathbf M_{t}\eqno{(5)}$$

参数$\beta$包含了点对的标识和步长的大笑。它在相同的点对中为正在不同的点对中为负。所以，对于相同的点对，优化向 $\mathbf C_{ij}$ 的方向进化而对于不同的点对则向相反的方向进化。每一对在梯度递减上的影响被概率所控制。

\subsection{线性判别度量}

Guillaumin从概率学的角度学习马氏距离度量。先验分类概率被看成判断相同（不同）的衡量标准，也就是说两张图片是否描述的是同一物体。给定一对$(i,j)$，先验概率模型为

$$p_{ij} = p(y_{ij} = 1 | x_{i}, x_{j}; M, b) = \sigma (b - d_{M}^2(x_i,x_j))\eqno{(6)}$$
其中$\sigma(z) = (1 + \exp(-z))^{-1}$是一个sigmoid函数，$b$是一个偏差项。所以为了确定矩阵$\mathbf M$，马氏矩阵不断地迭代使对数似然比最大：
$$L(M) = \sum_{ij}y_{ij}\ln(p_{ij}) + (1 - y_{ij})\ln(1 - p_{ij})\eqno{(7)}$$
通过梯度下降法得到的最大对数似然比，在向量$\mathbf C_{ij}$方向为相同分类对，而其相反方向为不同类。
$$\frac{\partial L(M)}{\partial M} = \sum_{ij}(y_{ij} - p_{ij})C_{ij} \eqno{(8)}$$

每对样本通过概率影响梯度下降的方向

如果我们概括一下以上所提到的尺度衡量方法的属性和特征，我们会发现两个共同点。第一，所有的方法依赖于迭代优化的过程，它在大规模数据集的情况下计算会变得花费很大。第二，如果我们比较一下  给出的不同方法中的更新规则，我们可以发现对于相同的点对，优化总是向$ \mathbf C_{ij}$ 的方向进行而对于不同的点对则是向$ \mathbf C_{ij}$ 的相反方向。在接下来，我们介绍一个不用迭代的公式，他可以在成对之间的差异上建立一个数学推导公式。这允许我们能够面对额外的可扩展性和等价约束能力的挑战。我们参数自由的方法在训练时非常的有效，可以继续研究持续增长的数据的尺度度量。


