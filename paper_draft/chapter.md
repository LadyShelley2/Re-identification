---
figureTitle: "图"
figPrefix: "图"

tableTitle: "表"
tblPrefix: "表"

eqnPrefix: "公式"
---

# 绪论
人脸识别是利用人脸识别算法对数字人脸信息进行身份识别的过程，完整的人脸识别流程为：从图片中检测人脸，然后对人脸进行识别，进而标识人脸身份。近几年来，卷积神经网络在模式识别领域取得了很好的效果，因此本文对卷积神经网络应用于人脸识别问题展开了研究和讨论。本章节主要包括人脸识别研究背景和意义，人脸识别系统的构成，人脸识别研究现状，深度学习研究现状等。

## 人脸识别背景和意义
随着互联网的迅猛发展，人们对信息安全的要求也越来越高。身份鉴定是保障信息安全的重要途径。每个人的生理特征具有唯一性、稳定性和客观性，通过识别生理特征可以唯一确定人的身份。近几年来，计算机视觉和人工智能发展迅猛，生物特征识别技术应运而生。通过传感器获取人的生理特征，再通过计算机、数学等知识进行分析，可以使计算机协助人们快速、便利地进行身份鉴定。

常见的生物特征识别技术有指纹识别、虹膜识别、人脸识别和行人再识别。其中人脸是身份信息最直观的表现方式，人脸识别技术是辨识自然人身份的重要手段，是其他计算机智能行为的基础。目前人脸识别已经应用于各行各业，并取得了很好的效果。在2008年中国举办奥运会期间，人脸识别技术便被用于奥运会的系统中，如今以人脸识别为基础的门禁系统更是随处可见。人脸识别的研究和应用给人们带来了极大的便利和安全保障。

在人脸识别实施过程中，样本信息往往受到各种可变因素的影响而发生变化，光照、角度、距离等因素都可能会带来样本信息的变化。因此如何能使识别技术更加准确、高效是研究的热点

## 人脸识别系统
一个完整的人脸识别系统应该包括人脸检测和人脸识别两个过程，如(#fig:sys-fd-fr)

![异或问题示意图](./pic/cqu.eps){#fig:sys-fd-fr}
人脸检测是指在图片中根据人脸结构特征，检测到人脸所在的区域并将人脸提取出来的过程。而人脸识别则是对比辨识以区分身份的过程，也是本文研究的重点，人脸识别的系统结构图如 @fig:sys-constru

![异或问题示意图](./pic/cqu.eps){#fig:sys-constru}

常见的方法包括基于几何特征、基于肤色模型和基于统计理论等方法。人脸识别方法主要有几何结构方法，子空间方法、局部特征方法和深度学习方法等。人脸识别系统的完整流程是：首先通过人脸检测技术将人脸从背景图中分离出来，然后对人脸进行特征提取，再通过相似度度量区分身份。本文内容集中于人脸识别，对人脸身份进行判断。

## 人脸识别研究现状

本文所讨论的人脸识别是通过对比人脸的特征值从而确定人脸身份的过程。人脸识别可以分为两个过程：特征提取和分类器选择。特征提取过程试图描述人脸信息的关键特征，而分类器的选择则直接影响了分类结果。目前用于人脸识别的方法主要有：几何结构发；基于子空间特征的方法，局部特征方法和深度学习方法。

几何结构法是通过利用一组几何特征矢量表示人脸面部拓扑结构的几何关系。该思想最初由Bledsoe与1966年提出，后IJ.Cox[@cox1996feature]，Huang[@huang1992human]等人改进。虽然基于几何结构的方法计算简单，但对特征点对齐要求非常高，从而大大限制了它的实用性。

基于子空间的方法的主要思想是将高维特征通过空间变换到一个低维的子空间中，使样本在低维空间中更容易分类。代表算法有：主成分分析PCA[@ 10]、线性判别分析LDA[@ 8 9]、独立分量分析(Independent Component Analysis, ICA)[@ 11]。而后，在这三种算法上进行改进的算法不断涌现，例如与核技术相结合的KPCA[@scholkopf1997kernel]，核2DPCA[@20]，核Fisherfaces[@ 24]。子空间的方法是在特征提取阶段所使用的方法，在保留人脸几何拓扑关系的同时也保留了部分局部特征。子空间是目前人脸识别最常用的特征方法，有计算量小，描述能力强，可分性好等优点。

局部特征方法主要思想是将人脸图像分解成多个局部特征，从而使特征分散，降低干扰因素的影响，局部特征方法较好地模拟了人类的识别能力，先对人脸的整体特征进行辨识，再对局部特征进行对比。代表方法有：由Ojala[33]等提出的LBP特征、Lowe[40 41]提出的SIFT特征、由Daugman等提出的Gabor特征[@ 25 26]等。LBP特征的思想是讲一个局部的中心像素的灰度值设为阈值，将周围的像素点与阈值作比较转换成0和1，从而表示出局部的纹理。LBP特征具有旋转不变性和灰度不变性等优点。2010年，X.Tan和Triggs Bill[@39]提出了局部三元模式(Local Ternary Pattern, LTP)进一步扩展了LBP，使之描述能力更强。SIFT特征全名尺度不变特征变换(Scale Invariant Feature Transform, SIFT)特征，SIFT特征具有尺度不变性、位移不变性、仿射不变性和旋转不变性。SIFT通常和其他方法配合使用。Ke Y[@42]于2004年提出PCA-SIFT方法；Gu J[@ 45]于2009年提出了结合K-Means的聚类匹配算法。Gobar小波能够同时有效的描述人脸图像的局部特征和整体特征。

深度学习的主要思想是模仿人类的识别过程。由于在人脸识别过程中，样本数据会受到光照、姿势的影响。但人类在认知和识别过程中却几乎不会受这些因素的影响，这使人们考虑是否可以通过模拟人类而使计算机同样排除这样的干扰，准确地进行识别和判断。G.Hinton等[@ 49]等利用贪心注册那个算法训练DSNs，该模型可以在没有标签的情况下学习图像的低阶特征。Marc Aurelio Ranzato等[@50]将门控马尔科夫随机场(MRF)作为DBNs的前端从而学习人脸图像的深度生成模型。Osadchy M 等[51]利用卷积网络进行人脸检测，该模型是将原始图像映射到低维子空间中。Sun Y[@52]等构建了基于三层卷积神经网络的级联回归结构。Huang G B 等[@53]通过卷积深度信念网络(CDSN)来学习到了多层次的特征。Nair 和 Hinton[@ 54]使用深度学习进行目标识别和人脸验证，然而他们提出的模型由于不具有平移不变性，所以需要人工校正眼坐标。Sun Y等[56]提出受限玻尔兹曼机(RBM)和混合卷积神经网络(ConvNet)的网络模型，该算法在LFW数据库中表现出较好的性能。Lin M 等[57]提出使用深度信念网络解决姿态变化带来的非线性问题。Chen等[@59]将图像分割成不重叠的图像块，分别输入深度神经网络进行训练以解决图像过大的问题。Zhu Z等[@ 60]提出FIP特征，以解决光照和姿态变化的问题。FIP特征显著减少类内差，比LBP，Gabor特征具有更好的鲁棒性。

与其他方法相比，利用神经网络和深度学习提取人脸特征，是对人脑工作原理的一种模拟，可以学习到更多人脸图像中的隐形特征，因此表现出很好的性能。

## 卷积神经网络研究现状
## 论文组织结构
人脸识别在现代社会中具有愈加重要的作用，高准确率和高效率是人脸识别追求的目标。深度学习凭借其对人脑工作原理的模拟，在模式识别中表现出良好的性能。本文将卷积神经网络运用于人脸识别并进行探索和研究。主要研究工作包括：

1. 通过阅读卷积神经网络的相关文献，仔细学习了感知器、多层感知器、后向传播算法等背景知识，理解了卷积神经网络的特征、结构以及应用方向等。学习了softmax分类器和支持向量机分类器等常用分类器的原理。
2. 将卷积神经网络应用于人脸识别，并通过对隐藏节点个数，过滤器个数等参数微调使之效果更好。
3. 将通过卷积神经网络提取的特征值，分别输入softmax分类器和支持向量机分类器中，对比结果并分析
4. 分别通过卷积神经网络提取的特征值与其他方法所提取的特征值输入至支持向量机，对比结果并分析。

本文中各章的内容组织结构如下：

第一章主要介绍人脸识别

# 相关知识
## 感知器

### 概念
感知器的思想于1957年由Frank Rosenblatt被提出。在机器学习中，感知器是用于处理监督学习下的二元分类问题。它的输入值是样本的特征向量$x$，输出值为二值函数$f(x)$，称为感知器的激活函数如：

\begin{displaymath}
y = \left\{ \begin{array}{ll}
 1 & \textrm{if $\omega \cdot x + b > 0$}\\
 0 & \textrm{otherwise}
  \end{array} \right.
\end{displaymath}

其中，$\omega$是输入向量$x$中各值对应权值所构成的向量，$\omega \cdot x$是求两者的内积，即$\sum_{i = 0}^{m}\omega_{i}x_{i}$，其中$m$是输入向量所包含数值的个数。$b$是偏移项，其值不取决于输入的任何一项，是可训练的。

### 构造方法
为了构造一个感知器，我们需要定义一些变量：

* $y = f(z)$：输入向量$z$到输出值的映射函数。
* $D = {(x_{1},x_{1},\ldots,(x_{s},d_{s}))}$：包含$s$个样本的训练集：
  其中：
    * $x_{j}$是n维输入向量，$x_{j,i}$表示第$j$个输入向量中的第$i$个特征值，$x_{j,0}=1$
    * $d_{j}$是输入向量对应的输出值
* $\alpha$：模型的学习率。其中$0<\alpha\le 1$

关于感知器结构中的权重，我们利用$\omega_{i}$来表示权重向量中的第$i$个值，将会与输入向量中的第$i$个特征值相乘；在前面我们定义$x_{j,0} = 1$，因此$w_{0}$对应的就是我们定义的偏移量$b$。利用$\omega_{i}(t)$表示第$t$次学习的权重

感知器的结构可表示为如图 @fig:slp_construct

![单层感知器结构图](./pic/cqu.eps){#fig:slp_construct}

### 学习算法
感知器的学习目的是寻找一个超平面能够使正负样本实例完全正确分开。样本的实际输出值与期望输出值的平均残差函数为：

$$\frac{1}{s}\sum_{j = 1}^{s}\lvert d_{j}-y_{j}(t)\rvert$$

感知器的优化目标应该使残差最小，理想的情况为0，但在实际应用中，往往有一定的容错率。可以定义容错阈值$\gamma$，当目标函数小于$\gamma$时，则停止学习。

学习过程如下：

1. 初始化权重$\omega$和阈值$\gamma$。权重可以被初始化为0或者其他的小随机数。
2. 对于训练集$D$中的每个样本$j$，我们对输入值$x_{j}$与期望输出$d_{j}$执行以下步骤：
    * 计算实际输出：$y_{j}(t)= f[w(t)\cdot x_{j}]$
    * 更新权值：对于所有$0\le i \le n$，计算$\omega_{i}(t+1) = \omega_{i}(t) + \alpha(d_{j} - y_{j}(t))x_{j,i}$
3. 训练至残差小于设定的阈值即可停止训练。

感知器是一个线性分类器，Frank Rosenblatt证明了如果一个两类模式是线性可分的，则一定存在一个超平面可以将它们分开。

## 多层感知器
感知器可以很好地解决两类线性分类问题，然而却无法解决非线性问题，例如 @fig:xor XOR问题：

![异或问题示意图](./pic/cqu.eps){#fig:xor}

单个感知器虽然无法解决异或问题，但多个感知器组合则可以实现复杂空间的分割。如 @fig:mlp-xor

![多感知器解决异或问题](./pic/cqu.eps){#fig:mlp-xor}

单个感知器可以将空间一分为二，在第一个感知器的基础上第二个感知器即可实现异或，因此多个感知器配合可以解决非线性分类问题。

### 网络结构
感知器是多层感知器的基本组成。多层感知器模拟人类神经的工作原理，将每一个感知器模拟人类神经的神经元的基础功能：来自外界的电信号通过突触传递给神经元，当细胞收到的信号综合超过一定阈值后，细胞被激活，通过轴突向下一个细胞发送电信号，完成对外界信息的加工。

多层感知器除输入和输出层以外，还包括至少一层以上的隐藏层，且层与层之间是全连接，即多层感知器与上一层的每一个感知器都有连接。多层感知器的结构图如 @fig:mlp-con

![多感知器解决异或问题](./pic/cqu.eps){#fig:mlp-con}

### 激活函数
感知器中的函数$f$称为激活函数，若激活函数为线性函数，则利用线性代数的知识，网络输出的任意层都可以被转换成标准的输入-输出两层模型。因此在多层感知器中，激活函数采用非线性函数以达到非线性分类的目的。

常用的激活函数为$y(v_{i}) = tanh(v_{i})$和$y(v_{i}) = (1+e^{-v_{i}})^{-1}$。前者是值域处于(-1,1)之间的双曲正切函数，后者是logistic函数，值域在(0,1)。前者是有后者变换而得到，两者形状很相似，两者的图像如 @fig:mlp-act

![多感知器解决异或问题](./pic/cqu.eps){#fig:mlp-act}

### 梯度下降法
梯度下降法是一种最优化算法，可以用来优化神经网络结构的参数，由于其原理是寻找最快下降的方向进行优化，因此也称为最快下降法。

梯度是标量场中某一点上指向标量场增长最快的方向，是一个向量场，梯度的长度即为该点最大的变化率。对于一个单变量的实值函数，梯度就是倒数。对于二元函数$f(x,y)$，若函数$f(x,y)$在平面区域$D$中具有一阶偏导数，则对于点$f(x,y)\in D$，梯度为$gradf(x,y) = \frac{\partial f}{\partial x}\vec{i} + \frac{\partial f}{\partial y}\vec{j}$，类似的对于三元函数$f(x,y,z)$，梯度为$\frac{\partial f }{\partial x }\vec{i} + \frac{\partial f }{\partial y }\vec{j} + \frac{\partial f }{\partial z }\vec{k}$，其中$\vec{i},\vec{j},\vec{k}$分别为x,y,z轴方向的单位向量。因此一个标量函数的梯度可以记为：$\nabla{\phi}$或者$grad\phi$，其中$\nabla$表示微分算子。

若实值函数$F(x)$在点$a$处可微且有定义，则函数$F(x)$在$a$点沿着梯度相反的方向$-\nabla{\phi}$下降最快。因而，如果$b = a - \gamma \nabla F(a)$成立，其中 $\gamma > 0$为一个足够小的数值，那么$F(a)\ge F(b)$。因此从初始值$x_{0}$出发，考虑如下序列$x_{0},x_{1},x_{2},\ldots$使得
$$x_{n+1} = x_{n} - \gamma_{n}\nabla F(x_{n}),n\ge 0$$
因此可以得到
$$F(x_{0})\ge F(x_{1})\ge F(x_{2})\ge\ldots,$$
最终使$(x_{n})$收敛到期望的极值，如 @fig:mlp-grad-1。

![多感知器解决异或问题](./pic/cqu.eps){#fig:mlp-grad-1}

梯度下降法的局限性在于由于初值设定随机，可能会陷入局部最小，而不是全局最小，如 @fig:mlp-grad-2

![多感知器解决异或问题](./pic/cqu.eps){#fig:mlp-grad-2}

### 后向传播学习
感知器的学习目标是使实际输出结果和期望输出结果之间的误差最小，通过后向传播实现。后向传播包括两个过程：传播和权重更新

* 传播
    * 向前传播：将数据输入至神经网络中，得到输出结果
    * 向后传播：利用实际输出与期望输出的误差计算对所有神经元的梯度
* 更新权重
    * 利用学习率计算权重更新的变化量
    * 将变化量更新至权重

后向传播的算法流程如 @fig:mlp-bp

![后向传播的算法流程](./pic/cqu.eps){#fig:mlp-bp}

以常见的平方误差为例，误差函数为：
$$ E = \frac{1}{2}(t-y)^{2}$$
其中$E$表示平方误差，$t$是训练数据的期望输出，$y$是实际输出，$\frac{1}{2}$是为了后续计算微分是时方便而加上的系数，不会影响最终优化结果。有感知器的计算原理可知，对于神经元$j$，它的输出$o_{j}$应为：
$$o_{j}=\phi(net_{j}) = \phi(\sum_{k = 1}^{n}\omega_{kj}o_{k})$$
激活函数的输入$net_{j}$等于网络上一层中各个感知器输出的加权之和，当计算输入数据后的第一层网络输出结果时，则是将输入数据进行加权求和。激活函数$\phi$是非线性且可微的，以logistic函数为例：
$$\phi(z) = \frac{1}{1+e^{-z}}$$
求导得：
$$\frac{d_{\phi}}{d_{z}}(z) = \phi(z)(1-\phi(z))$$
根据梯度下降法原理，我们采用链式求导求解误差对权值的偏微分：
$$\frac{\partial E}{\partial w_{ij}} = \frac{\partial E}{\partial o_{j}}\frac{\partial o_{j}}{\partial net_{j}}\frac{\partial net_{j}}{\partial \omega_{ij}}$${#eq:bp-d}
其中：
$$\frac{\partial net_{j}}{\partial \omega_{ij}} = \frac{\partial}{\partial \omega_{ij}}(\sum_{k = 1}^{n}\omega_{kj}o_{k})=o_{j}$${#eq:bp-d-1}

$$\frac{\partial o_{j}}{\partial net_{j}} = \frac{\partial}{\partial net_{j}}\phi(net_{j}) = \phi(net_{j})(1-\phi(net_{j}))$${#eq:bp-d-2}
对于输出层神经元，即$o_{j} = y$：

$$\frac{\partial E}{\partial o_{j}} = \frac{\partial E}{\partial y} = \frac{\partial}{\partial y}\frac{1}{2}(t-y)^2 = y-t$${#eq:bp-d-3-1}

对于非输出层神经元，我们可以将$E(o_{j})$是所有将神经元$j$的输出作为输入的神经元$L = u,v,\ldots,w$的误差函数：
$$\frac{\partial E}{\partial o_{j}} = \sum_{l\in L}(\frac{\partial E}{\partial net_{l}}\frac{\partial net_{l}}{\partial o_{j}} = \sum_{l\in L}(\frac{\partial E}{\partial o_{l}}\frac{\partial o_{l}}{\partial net_{l}}\omega_{jl}))$${#eq:bp-d-3-2}
将 @eq:bp-d，@eq:bp-d-1，@eq:bp-d-2，@eq:bp-d-3-1，@eq:bp-d-3-2 组合，得到：

$$\frac{\partial E}{\partial \omega_{ij}}= \delta_{j} o_{i}$$
其中：
$$
\delta_{j} =\frac{\partial E}{\partial o_{j}} \frac{\partial o_{j}}{\partial net_{j}} = \left\{ \begin{array}{ll}
 (o_{j}-t_{j})o_{j}(1-o_{j})) & \textrm{$j$ 为输出层神经元，}\\
 (\Sigma_{l\in L}\delta_{l}\omega_{jl})o_{j}(1-o_{j}) & \textrm{ $j$ 为非输出层神经元}
  \end{array} \right.
$${#eq:bp-d-delta}

此时，可以对权重进行更新了，为了使目标值向降低的方向优化，我们需要乘上-1，权重更新的步长由学习率$\alpha$确定：

$$
\Delta\omega_{ij} = -\alpha\frac{\partial E}{\partial \omega_{ij}}= \left\{ \begin{array}{ll}
 -\alpha o_{j}(o_{j}-t_{j})o_{j}(1-o_{j})) & \textrm{$j$ 为输出层神经元，}\\
 -\alpha o_{j}(\Sigma_{l\in L}\delta_{l}\omega_{jl})o_{j}(1-o_{j}) & \textrm{ $j$ 为非输出层神经元}
  \end{array} \right.
$${#eq:bp-d-delta-weight}

# 卷积神经网络
## 概述

卷积神经网络（Convolutional Neural Networks）是多层感知器的变体，是目前模式识别的研究热点。它的提出源于对猫的视觉皮层细胞的研究，1962年Hubel和Wiesel提出了感受野（receptive field）的概念，1984年此概念在日本学者Fukushima提出的神经认知机（neocognitron）中首次被应用。神经认知机将一个视觉模式分解成许多子模式（特征）然后进入分层递阶式相连的特征平面进行处理，尝试在物体有位移或轻微变形的时候，也能完成识别。神经认知机可以看做第一个卷积神经网络的实现。

卷积神经网络的主要特点体现在两个方面：局部连接，权值共享和子采样。局部连接是指层与层神经元之间的连接采用局部连接代替全连接；权值共享是指同一层中神经元之间的连接权值是共享的；子采样是对得到的特征图进行特征采样。三者使卷积神经网络在很大程度上降低了参数数量，从而使网络的复杂度降低。由于其结构与生物神经网络非常相似，即使输入的图像不做任何预处理，卷积神经网络的识别效果也比较显著，同时避免了繁琐的特征提取的过程。
本章将详细介绍卷积神经网络的基本思想、拓扑结构和常用分类器。

## 主要思想
根据Hubel和Wiesel对猫初级视皮层的研究，生物的初级视皮层包括简单细胞和复杂细胞，简单细胞主要负责感知其感受野内的特定边缘刺激，而复杂细胞则以简单细胞的输出为输入，并负责以更大的感受野来感受边缘刺激。根据简单细胞和复杂细胞的工作原理，卷积神经网络主要采用三种结构来进行视皮层的模拟：局部连接、权值共享以及子采样。

### 局部连接
局部连接是指在相邻层之间不使用全连接而使用局部连接，从而不仅减少了需要训练的参数数量，而且利用了图像的局部特征信息。

如 @fig:cnn-locallink-1 所示，图a为全连接，图b为局部连接。假设图片有$1000\times1000$个像素的图片，有一百万的隐层神经元，全连接需要每一个隐层神经元连接到图像的每一个像素点，有$100\times 100\times 1000000 = 10^{12}$个连接，也就需要$10^{12}$个参数。局部连接则只需要每个节点只与其感受野中的像素点进行相连，假设其感受野为$10\times 10$，则一百万个隐层神经元就只要$10\times 10\times 100000 = 10^{8}$个权值参数，权值参数的个数减少四个数量级。因此局部连接减少了所需训练的权值参数。

![后向传播的算法流程](./pic/cqu.eps){#fig:cnn-locallink-1}

如 @fig:cnn-locallink-2所示，每一层神经元只与其前一层的神经元存在局部连接，第I层的神经元连接了I-1层神经元的3个相邻的神经元，第I+1层与第I层的连接也有类似的规则，I+1层的神经元虽然相对于第I层的接受域宽度为3，但其相对于第I-1层的接受域却为5，这种结构经过多个层堆叠在一起之后，会使得过滤器逐渐成为全局，但却包含了低层的很多局部信息，因此局部连接可以利用图像的局部信息。通常在实际应用中，利用多个局部连接的过滤器可以利用图像的多种局部特征。

![后向传播的算法流程](./pic/cqu.eps){#fig:cnn-locallink-2}

### 权值共享
在上节例子中通过局部连接，所需训练参数有了数量级上的减少，但仍然需要训练$10_{8}$个参数，这意味着如果想要得到有意义的参数，则需要样本容量大于$10_{8}$，而如此庞大的样本容量常常不易达到，即便可以达到，网络的结构也会异常复杂，训练结果精确度也不会很高，而权值共享则可以很好地解决这个问题。

权值共享是指在相邻层神经元连接时都采用相同的权值，如 @fig:cnn-sharedweight-1所示，不同的线条形状代表不同的权值，相同的线条形状代表相同权值，则上节例子中，若局部连接的感受野为10*10，则只需要100个权值，因此大大降低了所需权值数量。此外，由于权值共享使权值以同样的方向和距离出现，因此权值共享使卷积神经网络具有平移不变性。

![后向传播的算法流程](./pic/cqu.eps){#fig:cnn-sharedweight-1}

通过局部连接和权值共享，并以卷积的方式在输入的每个位置提取输入的局部特征，卷积神经网络有效模拟了视皮层中的简单细胞。

### 子采样
子采样操作是对得到的特征图进行特征映射（特征采样），在水平和竖直的方向利用$w\times w$的连续子区域以$s$为步长进行特征映射，其中$1\le s\le w$，当$s = w$时，采样子区域之间没有重叠部分，否则，采样区域间有重叠部分。常用的映射方法是最大值映射和平均值映射，即在$w\times w$的子区域中，选取最大值或者计算子区域中的平均值作为该区域的映射值。如@fig:cnn-sample-1所示，特征图的大小为$4\times 4$，若采用$2\times 2$的连续子区域以2为步长进行子采样，采样后的特征图为$(4/2)\times (4/2)$，即$3 \times 3$。可以看出子采样减少了神经元的数目，相同的神经元个数代表了更大的感受野，很好地模拟了视皮层复杂细胞。

![后向传播的算法流程](./pic/cqu.eps){#fig:cnn-sample-1}

## 网络拓扑结构
卷积神经网络的传统模型是由多层特征提取阶段与一个分类器组成的结构，输入的特征在经过多层的特征映射学习到高层特征之后，利用在最后一个阶段得到的特征被输入分类器进行分类。通常在应用中，卷积神经网络一共有1-3个特征映射阶段，每个特征映射阶段包括卷积层和子采样层。
LeNet-5是一个用于手写体识别的网络结构，本节将以此为例展开介绍，LeNet-5结构如 @fig:cnn-topol-lenet。

![后向传播的算法流程](./pic/cqu.eps){#fig:cnn-topol-lenet}


### 卷积层
卷积层是卷积神经网络的重要组成部分。卷积层将前一层的一个或者多个特征图与一个或者多个卷积核进行卷积操作，输出特征图中的相邻神经元共享大部分的输入特征图中的神经元。对于一个大小$m\times n$的特征图，用大小$k\times k$的卷积核进行卷积操作，输出特征图的大小为$(m-k=1)\times (k-k+1)$。如 @cnn-topol-c 所示，一个大小为$5\times 5$的卷积核在图像大小为$8\times 8$上进行卷积，得到的输出特征图像为$4 \times 4$

![重写，有卷积推导过程](./pic/cqu.eps){#fig:cnn-topol-c}

一般在卷积操作之后，会在结果上加一个偏置参数，此偏置参数是可训练的。此外，为了使神经网络具有非线性的拟合性能，需要一个非线性的激活函数，通过该函数映射后最终得到卷基层的输出特征图。

以LeNet-5结构为例，第一层、第三层和第五层为卷积层，在其第一个卷积层中，输入图像为原始图像，大小为$28\times 28$，8个大小为$5 \times 5$的卷积核卷积后得到8张大小为$28\times 28$的特征图。在第三层中同样采用$5 \times 5$的卷积核卷积得到20张大小为$10\times 10$的特征图，第五层为一个全连接层，产生一个与原始输入图像对应的特征向量，也是要输入到分类器进行分类的向量。

### 子采样层
子采样层对卷积层的输出特征图进行采样，采样区域的宽度和高度可以根据实际情况进行调节。在采样子区域没有重叠的情况下，一张大小为$m\times n$的输入特征图，经过$w\times h$的尺度进行采样，则得到的图像大小应为$(m/w)\times (n/h)$

在LeNet-5中第二层和第四层为子采样层，在第二层中，通过$2\times 2$的尺度进行子采样，将大小为$28\times 28$的图像采样为$14\times14$的图像，同理在第四层中，将大小为$10\times 10$的输入图像采样为$5\times 5$的输出图像。

### 分类器
分类器是将得到最终输出的特征向量进行分类的分类器。常用的分类器有logistic回归模型以及其扩展softmax分类或者一层或两层的神经网络。在LeNet-5中用的是softmax 分类器。

以上就是基本的卷积神经网络结构，在实际应用中，卷积和子采样的层数、卷积过滤器的维数、子采样采样子区域的维数等参数都是可调节的。可以根据具体情况提出有效的网络结构。

## 常用分类器
### softmax分类器
### 支持向量机
# 基于卷积神经网络的人脸识别
## YaleB数据集
## 网络设计
## 实验结果与分析
#基于CNN-SVM算法的人脸识别
## CNN-SVM算法
## 参数设置
## 实验结果与分析
# 总结和展望
## 全文总结
## 未来展望